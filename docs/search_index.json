[
["index.html", "Data Science in Education: A Step-by-Step Guide Preface 0.1 Getting Data Topics 0.2 Analyzing Data Topics 0.3 Presenting Data Analysis Topics", " Data Science in Education: A Step-by-Step Guide Will Doyle 2017-08-23 Preface We have entered a time in which vast amounts of data are more widely available than ever before. At the same time, a new set of tools has been developed to analyze this data and provide decision makers with information to help them accomplish their goals. Those who engage with data and interpret it for organizational leaders have taken to calling themselves data scientists, and their craft data science. Other terms that have come into vogue are “Big Data,” “Predictive Analytics” and “Data Mining.” These can seem to be mysterious domains. The point of this class is to demystify much of this endeavor for individuals who will be organizational leaders. The class is structured around developing students’ skills in three areas: getting data, analyzing data to make predictions, and presenting the results of analysis. For each area, the subtopics are as follows: 0.1 Getting Data Topics Tools of the Trade: R and Rstudio, git and Github Working with pre-processed data and flat files Getting data from the web: webscraping, using forms, using Application Programming Interfaces Using databases 0.2 Analyzing Data Topics Conditional means Regression Supervised learning: classification Unsupervised learning: K-means and nearest neighbors clustering Evaluating multiple models 0.3 Presenting Data Analysis Topics Descriptives: histograms, density plots, bar plots, dot plots Scatterplots Lattice graphics and small multiples Maps Interactive Graphics "],
["intro.html", "1 Welcome to Data Science! 1.1 Introductions 1.2 Installing R 1.3 Installing Rstudio 1.4 Installing git 1.5 Getting set up on GitHub 1.6 Initializing repos 1.7 Installing Github Desktop 1.8 Integrating Rstudio with GitHub 1.9 Cloning down your repo 1.10 Linking Github and Rstudo 1.11 Yes We Code! Running R Code 1.12 .Rmd files 1.13 Github: save, stage, commit, push 1.14 Your first commit: hello_world.Rmd", " 1 Welcome to Data Science! Today, we’ll be working on getting you set up with the tools you will need for this class. Once you are set up, we’ll do what we’re here to do: analyze data! Here’s what we need to get done today: Introductions Installing R Installing Rstudio Installing git Getting set up on GitHub Initializing everyone’s github repos hello_world.Rmd 1.1 Introductions We need three basic sets of tools for this class. We will need R to analyze data. We will need RStudio to help us interface with R and to produce documentation of our results. Last, we will need git and GitHub to communicate our results to the wider world. 1.2 Installing R R is going to be the only programming language we will use. R is an extensible statistical programming environment that can handle all of the main tasks that we’ll need to cover this semester: getting data, analyzing data and communicating data analysis. If you haven’t already, you need to download R here: https://cran.r-project.org/. 1.3 Installing Rstudio When we work with R, we communicate via the command line. To help automate this process, we can write scripts, which contain all of the commands to be executed. These scripts generate various kinds of output, like numbers on the screen, graphics or reports in common formats (pdf, word). Most programming languages have several I ntegrated D evelopment E nvironments (IDEs) that encompass all of these elements (scripts, command line interface, output). The primary IDE for R is Rstudio. If you haven’t already, you need to download Rstudio here: https://www.rstudio.com/products/rstudio/download2/. You need the free Rstudio desktop version. 1.4 Installing git git is a version control program. A standard problem in programming is how to track the changes that have been made to a project. These challenges are well-known to anyone who has had to work on a complex document, particularly with collaborators. Because these problems are particularly severe in programming, they developed a set of tools called version control. These will keep track of changes you make to a file, and record why you made the change. Download git here and accept all of the defaults on installation: https://git-scm.com/downloads 1.5 Getting set up on GitHub GitHub is online hosting service that is widely used by programmers. It allows you to easily share your work with the world. It is integrated with git, so version control is easy to do. If you haven’t already, go to: https://github.com and sign up for free to get a username. Share the username with me as soon as you have one. 1.6 Initializing repos Everyone in the class will need a repository (repo from now on) on our GitHub organization. All I need from you is your username on GitHub, then I can add your repo to our organization. 1.7 Installing Github Desktop Github desktop is a G raphical U ser I nterface for git. We won’t use it much, but you will need to, particularly to “clone” your git repository. Download github desktop: https://desktop.github.com/. Choose the beta version, which appears to be working nicely. Accept all of the defaults on installation. Unzip and open up the github app. Then using the app, sign into github using your username and ID. 1.8 Integrating Rstudio with GitHub To integrate Rstudio with GitHub, you need to enable git as your version control within Rstudio. Go to Preferences–&gt;Git/SVN and make sure the “enable version control” box is checked. Make sure that you can see a path to git. Further instructions (if needed) are here. 1.9 Cloning down your repo First in RStudio, you’ll need to generate an SSH key. This will let GitHub know it’s safe to talk to your computer. In RStudio, go to Preferences–&gt; Git/SVN, then click on “Create RSA Key”. Once that’s done, you can click on the blue text that says “View public key.” Copy all of the text in the public key. Now go to https://github.com and in the upper right hand corner by your avatar, click on Settings. Go to SSH and GPG keys. Click “New SSH Key” and paste in the text you copied from Rstudio. Name the key, and you should be all set. Now, go to your repo for this class. Everyone’s repo is in the hoddatasci organization, and usese the naming convention student_&lt;yourlastname&gt;. Click the green button that says “clone or download,” then copy the link provided by clicking on the clipboard next to the link. N.B. in this class, and generally in programming, when you see &lt;text&gt; that means that you need to substitute something in. Now go to Github desktop, and click the “clone a repository.” Paste in the link you just copied, choose a location on your computer, and proceed. It should show you a link to your repository. 1.10 Linking Github and Rstudo From RStudio, you’ll need to click New Project–Existing Directory– then choose the directory where you just downloaded the gihub repository. Name the project “central”. Choose a good spot on your computer for the project files– wherever you usually keep class directories. This project will be the only place you need to work for this class the entire semester. Once you click “create project,” you should see a “git” tab in your environment. Open up the file named 01-intro.Rmd and take a look. 1.11 Yes We Code! Running R Code The following code chunk will be our first use of R in this class. We’re going to grab some data that’s part of the college scorecard and do a bit of analysis on it. 1.12 .Rmd files Open the 01-Intro.Rmd file. In Rstudio, go to File–&gt;Open, then find the 01-Intro.Rmd file in the directory. .Rmd files will be the only file format we work in this class. .Rmd files contain two basic elements: Script that can be interpreted by R. Text that can be read by humans. From a .Rmd file you can generate html documents, pdf documents, word documents, slides . . . lots of stuff. All class notes will be in .Rmd. All assignments will be turned in as .Rmd files, and your final project? You guessed it, .Rmd. In the 01-Intro.Rmd file you’ll notice that there are three open single quotes in a row, like so: ``` This indicates the start of a “code chunk” in our file. The first code chunk that we load will include a set of programs that we will need all semester long. When we say that R is extensible, we mean that people in the community can write programs that everyone else can use. These are called “packages.” In these first few lines of code, I load a set of packages using the library command in R. The set of packages, called tidyverse were written by Hadley Wickham and play a key role in his book. To install this set of packages, simply type in install.packages(&quot;tidyverse&quot;) at the R command prompt. To run the code below in R, you can: Press the “play” button next to the code chunk In OS X, place the cursor inside the code chunk and hit Command+Enter. In Windows, hit Control+Enter ## Clear environment rm(list=ls()) ## Get necessary libraries-- won&#39;t work the first time, because you need to install them! library(tidyverse) Now we’re ready to load in data. The data frame will be our basic way of interacting with everything in this class. The sc data frame contains information from the college scorecard on 127 different colleges and univeristies. However, we first need to make sure that R is looking in the right place. When you opened up your project, Rstudio automagically took you to the directory for that project, so you should be ok!. ## Load in the data load(&quot;college.Rdata&quot;) Here are the variables in the college.Rdata dataset: Variable Name :Definition unitid: Unit ID instnm: Institution Name stabbr: State Abbreviation year: Year control: control of institution, 1=public, 2= private non-profit, 3=private for-profit preddeg: predominant degree, 1= certificate, 2= associates, 3= bachelor’s, 4=graduate adm_rate: Proportion of Applicants Admitted sat_avg: Midpoint of entrance exam scores, on SAT scale, math and verbal only costt_4a: Average cost of attendance (tuition and room and board less all grant aid) debt_mdn: Median debt of graduates md_earn_ne_pg: Earnings of graduates who are not enrolled in higher education, six years after graduation Looking at datasets We can look at the first few rows and columns of sc by typing in the data name. We can look at the whole dataset using View. ## What does this data look like? Look at the first few rows, first few variables sc ## # A tibble: 127 × 9 ## unitid instnm stabbr year ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 446048 Ave Maria University FL 2009 ## 2 443410 DigiPen Institute of Technology WA 2009 ## 3 441186 San Diego State University-Imperial Valley Campus CA 2009 ## 4 404338 Schiller International University FL 2009 ## 5 169442 College for Creative Studies MI 2009 ## 6 168342 Williams College MA 2009 ## 7 167057 The New England Conservatory of Music MA 2009 ## 8 166027 Harvard University MA 2009 ## 9 166683 Massachusetts Institute of Technology MA 2009 ## 10 168148 Tufts University MA 2009 ## # ... with 117 more rows, and 5 more variables: adm_rate &lt;dbl&gt;, ## # sat_avg &lt;dbl&gt;, costt4_a &lt;int&gt;, debt_mdn &lt;dbl&gt;, md_earn_wne_p6 &lt;int&gt; #View(sc) Filter, Select, Arrange In exploring data, many times we want to look at smaller parts of the dataset. There are three commands we’ll use today that help with this. -filter selects only those cases or rows that meet some logical criteria. -select selects only those variables or coloumns that meet some criteria -arrange arranges the rows of a dataset in the way we want. For more on these, please see this vignette. Let’s grab just the data for Vanderbilt, then look only at the average test scores and admit rate. ## Where are we? sc%&gt;%filter(instnm==&quot;Vanderbilt University&quot;) ## # A tibble: 1 × 9 ## unitid instnm stabbr year adm_rate sat_avg costt4_a ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 221999 Vanderbilt University TN 2009 0.2015 1430 52303 ## # ... with 2 more variables: debt_mdn &lt;dbl&gt;, md_earn_wne_p6 &lt;int&gt; sc%&gt;%filter(instnm==&quot;Vanderbilt University&quot;)%&gt;%select(instnm,adm_rate,sat_avg ) ## # A tibble: 1 × 3 ## instnm adm_rate sat_avg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Vanderbilt University 0.2015 1430 ## Just colleges with low admit rates: show admit rate and sat scores, arrange in a pleasing way sc%&gt;%filter(adm_rate&lt;.1)%&gt;%select(instnm,adm_rate,sat_avg)%&gt;%arrange(sat_avg,adm_rate) ## # A tibble: 6 × 3 ## instnm adm_rate sat_avg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Cooper Union for the Advancement of Science and Art 0.0735 1336 ## 2 Stanford University 0.0797 1436 ## 3 Harvard University 0.0719 1468 ## 4 Yale University 0.0856 1475 ## 5 Dell&#39;Arte International School of Physical Theatre 0.0000 NA ## 6 The Juilliard School 0.0711 NA ## Just colleges with low admit rates: order by sat scores (- sat_avg gives descending) sc%&gt;%filter(adm_rate&lt;.1)%&gt;%select(instnm,adm_rate,sat_avg)%&gt;%arrange(-sat_avg) ## # A tibble: 6 × 3 ## instnm adm_rate sat_avg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Yale University 0.0856 1475 ## 2 Harvard University 0.0719 1468 ## 3 Stanford University 0.0797 1436 ## 4 Cooper Union for the Advancement of Science and Art 0.0735 1336 ## 5 The Juilliard School 0.0711 NA ## 6 Dell&#39;Arte International School of Physical Theatre 0.0000 NA ## New cut sc%&gt;%filter(adm_rate&gt;.3)%&gt;%select(instnm,sat_avg,md_earn_wne_p6,stabbr)%&gt;%arrange(stabbr,-sat_avg)%&gt;%print(n=100) ## # A tibble: 72 × 4 ## instnm sat_avg ## &lt;chr&gt; &lt;dbl&gt; ## 1 Henderson State University 1048 ## 2 University of Arkansas at Pine Bluff 784 ## 3 University of Advancing Technology NA ## 4 Scripps College 1336 ## 5 University of California-San Diego 1247 ## 6 Pepperdine University 1230 ## 7 California Polytechnic State University-San Luis Obispo 1205 ## 8 San Diego State University 1050 ## 9 Hope International University 975 ## 10 San Diego State University-Imperial Valley Campus 825 ## 11 The Art Institute of California-Argosy University San Francisco NA ## 12 Colorado College 1326 ## 13 Connecticut College NA ## 14 George Washington University 1276 ## 15 Delaware State University 868 ## 16 Ave Maria University 1104 ## 17 Florida International University 1102 ## 18 Brenau University 965 ## 19 National Louis University NA ## 20 Grambling State University 851 ## 21 Brandeis University 1367 ## 22 Boston College 1343 ## 23 The New England Conservatory of Music NA ## 24 The Boston Conservatory NA ## 25 College for Creative Studies 970 ## 26 Rocky Mountain College 1042 ## 27 University of North Carolina at Chapel Hill 1305 ## 28 Campbell University 1018 ## 29 Shaw University NA ## 30 Wake Forest University NA ## 31 Nebraska Methodist College of Nursing &amp; Allied Health 1030 ## 32 New Jersey City University 835 ## 33 Colgate University 1357 ## 34 University of Rochester 1334 ## 35 New York University 1317 ## 36 SUNY College at Geneseo 1310 ## 37 SUNY at Binghamton 1265 ## 38 Stony Brook University 1193 ## 39 Marist College 1161 ## 40 State University of New York at New Paltz 1115 ## 41 SUNY Oneonta 1098 ## 42 SUNY Institute of Technology at Utica-Rome 1086 ## 43 CUNY Queens College 1060 ## 44 CUNY Brooklyn College 1035 ## 45 CUNY City College 1015 ## 46 CUNY Lehman College 910 ## 47 Bard College NA ## 48 St Lawrence University NA ## 49 Manhattan School of Music NA ## 50 Oberlin College 1360 ## 51 Ohio Christian University 901 ## 52 Central State University 759 ## 53 Wilberforce University NA ## 54 Carnegie Mellon University 1392 ## 55 Lehigh University 1300 ## 56 Lincoln University of Pennsylvania 812 ## 57 University of Puerto Rico-Rio Piedras 1206 ## 58 Bayamon Central University NA ## 59 Universidad del Sagrado Corazon NA ## 60 Universidad Central Del Caribe NA ## 61 Inter American University of Puerto Rico-Bayamon NA ## 62 Inter American University of Puerto Rico-Arecibo NA ## 63 Inter American University of Puerto Rico-Guayama NA ## 64 Rhode Island School of Design 1244 ## 65 Claflin University 895 ## 66 Hardin-Simmons University 1047 ## 67 South University-The Art Institute of Dallas NA ## 68 Southwestern Assemblies of God University NA ## 69 College of William and Mary 1339 ## 70 University of Virginia-Main Campus 1332 ## 71 University of Richmond 1261 ## 72 DigiPen Institute of Technology 1194 ## # ... with 2 more variables: md_earn_wne_p6 &lt;int&gt;, stabbr &lt;chr&gt; Quick Exercise Choose a different college and two different things about that college. Summarizing Data ## What&#39;s the average median debt? sc%&gt;%summarize(mean_debt=mean(debt_mdn,na.rm=TRUE)) ## # A tibble: 1 × 1 ## mean_debt ## &lt;dbl&gt; ## 1 13428.78 Quick Exercise Summarize the average entering SAT scores in this dataset. Combining Commands We can also combine commands, so that summaries are done on only a part of the dataset. Below, I summarize median debt for selective schools, and not very selective schools. ## What&#39;s the average median debt for very selective schools? sc%&gt;%filter(adm_rate&lt;.1)%&gt;%summarize(mean_debt=mean(debt_mdn,na.rm=TRUE)) ## # A tibble: 1 × 1 ## mean_debt ## &lt;dbl&gt; ## 1 8264.5 ## And for not very selective schools? sc%&gt;%filter(adm_rate&gt;.3)%&gt;%summarize(mean_debt=mean(debt_mdn,na.rm=TRUE)) ## # A tibble: 1 × 1 ## mean_debt ## &lt;dbl&gt; ## 1 14120.92 Quick Exercise Calculate average earnings for schools where SAT&gt;1200 Grouping Data Another powerful tool is being able to calculate characteristics for various groups. For example, what are the average earnings for the three different types of colleges (public, private non-profit, private for-profit) in the dataset? # # sc%&gt;%group_by(control) # %&gt;% # summarize(mean_earnings=mean(md_earn_wne_p6)) # # sc%&gt;%group_by(control)%&gt;% # summarize(mean_debt=mean(debt_mdn)) Plotting Data The last basic tool for looking at a dataset is plotting the data. ## Plotting: bivariate gg&lt;-ggplot(data=sc,aes(x=sat_avg,y=adm_rate)) gg&lt;-gg+geom_point() gg&lt;-gg+geom_smooth() gg ## `geom_smooth()` using method = &#39;loess&#39; ## Warning: Removed 27 rows containing non-finite values (stat_smooth). ## Warning: Removed 27 rows containing missing values (geom_point). ## Univariate descriptives gg&lt;-ggplot(data=sc,aes(adm_rate)) gg&lt;-gg+geom_density() gg Quick exercise Replicate the above plots, but put cost of attendance on the y axis. 1.13 Github: save, stage, commit, push When working with files in your directory, there are three basic steps you can take when using Git as your version control. Saving the file means it is only available to you, on your computer. You should save files as you are working on them. Staging the file means that you would like git (and github) to keep track of the changes you’re making to the file. Stage the file by clicking the “staged” box next to it in the Git tab in Rstudio. Committing means that you would like to keep the version of the file you generated. Think of this like emailing it to group members. It doesn’t mean that it’s done, but it does mean it’s in a state that you would like to have a record of as you move forward. To commit a file in Rstudio, click the “commit” button. You will be prompted to add a commit message. There’s actually quite a lot of thought about what goes into a commit message. The best idea for now is simply to state why you did what you did, and avoid profanity or any demeaning language. Pushing means that you will send the file and the record of all of the changes made to the file to GitHub. To push, click the “push” button in the Git tab in Rstudio. You should do this everytime you finish a working session, at an absolute minimum. 1.14 Your first commit: hello_world.Rmd For today, I want you to create a file called 01-assignment_&lt;lastname&gt;.Rmd in your github repo for assignments. It should contain the following elements: A sentence that says “Hello, World” R output that summarizes one of the variables in the colllege.Rdata dataset R output that shows a scatterplot for two of the variables in the college.Rdata dataset. Lucky for you this is is also your first assignment! Submit it under assignments, using the format 01-assignment_&lt;lastname&gt;.Rmd. All assignments should be turned in using this format. Since my last name is Doyle, I would use 01-assignment_doyle.Rmd as my file name. Unless your name is also Doyle, you should use a different name. Stretch Items If you have extra time, you can do the following: Calculate the average earnings for individuals at the most selective colleges, then compare that with individuals at the least selective colleges in the dataset. Find a way to determine whether colleges with very high SAT scores tend to be larger or smaller than colleges with low SAT scores. Plot the relationship between cost and debt. What do you see? Does this surprise you? Now, provide separate plots for cost and debt by control of the institution. Save, commit and push the assignment file to the assignments directory. "],
["analyzing-data-part-1-conditional-means.html", "2 Analyzing Data, Part 1: Conditional Means 2.1 Dataset for this week 2.2 Dependent Variable 2.3 Unconditional Means 2.4 Conditional Means With One Predictor Variable 2.5 Conditional Means with Multiple Predictors 2.6 Final Comparison 2.7 Applications of the Conditional Mean", " 2 Analyzing Data, Part 1: Conditional Means The conditional mean will be our first predictive algorithm. Conditional means answer the question: “Given what we know about a certain case, what can expect to see on average?” The conditional mean is a powerful tool that is typically quite easy to explain to decision-makers. We’ll go through the following steps: Computing and plotting unconditional means Computing and plotting conditional means using a single predictor. Computing and plotting conditional means using multiple predictors. 2.1 Dataset for this week We will be working with a dataset put together by the census bureau that summarizes the characteristics of the 3,088 counties in the United States. load(&quot;pd.Rdata&quot;) pd ## # A tibble: 3,088 × 55 ## fips pop2013 pop2010_base popchange_pc pop2010 popu5 popu18 pop65p ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01001 55246 54571 1.2 54571 6.1 25.4 13.5 ## 2 01003 195540 182265 7.3 182265 5.7 22.4 18.1 ## 3 01005 27076 27457 -1.4 27457 5.8 21.1 15.9 ## 4 01007 22512 22919 -1.8 22915 5.3 21.3 14.3 ## 5 01009 57872 57322 1.0 57322 6.1 23.8 16.4 ## 6 01011 10639 10915 -2.5 10914 6.3 21.0 14.6 ## 7 01013 20265 20946 -3.3 20947 6.1 23.3 17.7 ## 8 01015 116736 118572 -1.5 118572 5.8 22.2 15.5 ## 9 01017 34162 34170 0.0 34215 5.9 21.4 17.8 ## 10 01019 26203 25986 0.8 25989 4.8 20.7 20.1 ## # ... with 3,078 more rows, and 47 more variables: female_pc &lt;dbl&gt;, ## # white_pc &lt;dbl&gt;, black_pc &lt;dbl&gt;, am_ind_pc &lt;dbl&gt;, asian_pc &lt;dbl&gt;, ## # hawaii_pi_pc &lt;dbl&gt;, twomore_race_pc &lt;dbl&gt;, hispanic_pc &lt;dbl&gt;, ## # white_non_hispanic_pc &lt;dbl&gt;, same_house_pc &lt;dbl&gt;, ## # foreign_born_pc &lt;dbl&gt;, other_eng_home_pc &lt;dbl&gt;, hs_grad_pc &lt;dbl&gt;, ## # coll_grad_pc &lt;dbl&gt;, veterans &lt;int&gt;, travel_time &lt;dbl&gt;, ## # housing_units &lt;int&gt;, homeown_rate &lt;dbl&gt;, house_unit_multi &lt;dbl&gt;, ## # median_home_val &lt;int&gt;, households &lt;int&gt;, person_per_hh &lt;dbl&gt;, ## # per_capita_inc &lt;int&gt;, median_hh_inc &lt;int&gt;, ## # persons_below_poverty &lt;dbl&gt;, pv_nonfarm &lt;int&gt;, ## # pv_nonfarm_employ &lt;int&gt;, pv_nonfarm_employ_ch &lt;dbl&gt;, ## # nonemployer_est &lt;int&gt;, firms &lt;int&gt;, firms_black_own_pc &lt;dbl&gt;, ## # firms_amind_own_pc &lt;dbl&gt;, firms_asian_own_pc &lt;dbl&gt;, ## # firms_hawaii_pi_own_pc &lt;dbl&gt;, firms_hispanic_own_pc &lt;dbl&gt;, ## # firms_female_own_pc &lt;dbl&gt;, manufacture_ship &lt;dbl&gt;, wholesale &lt;dbl&gt;, ## # retail &lt;dbl&gt;, retail_percap &lt;int&gt;, hospitality &lt;int&gt;, ## # bldg_permits &lt;int&gt;, land_area &lt;dbl&gt;, pop_per_square &lt;dbl&gt;, ## # county &lt;chr&gt;, percapinc.2010 &lt;dbl&gt;, percapinc.2012 &lt;dbl&gt; The codebook for this dataset is stored as another dataset, labels_explain. The first column in this dataset is variable names, the second column is a full explanation of that variable. ## Full explanation of data load(&quot;lab_explain.Rdata&quot;) lab_explain ## # A tibble: 51 × 2 ## varname label ## &lt;fctr&gt; &lt;fctr&gt; ## 1 pop2013 Population, 2013 estimate ## 2 pop2010_base Population, 2010 (April 1) estimates base ## 3 popchange_pc Population, percent change - April 1, 2010 to July 1, 2013 ## 4 pop2010 Population, 2010 ## 5 popu5 Persons under 5 years, percent, 2013 ## 6 popu18 Persons under 18 years, percent, 2013 ## 7 pop65p Persons 65 years and over, percent, 2013 ## 8 female_pc Female persons, percent, 2013 ## 9 white_pc White alone, percent, 2013 ## 10 black_pc Black or African American alone, percent, 2013 ## # ... with 41 more rows Quick Exercise: Find the per capita income and the percent of the population with a bachelor’s degree for the county you’re from. 2.2 Dependent Variable Our working example will be based on predicting income in a given county. Suppose we want to know what income level we can expect for a geographic area based on observed characteristics, such as the proportion of the population with a bachelor’s degree. How would we predict the income based on what we know about the geographic area? Let’s begin by plotting the data to see what it looks like. To do this I need to first rank the counties by income. To create a rank variable that will be stored in the pd dataset, I use the mutate command. This creates a variable based on some calculation then stores it in the same dataset. I’m then going to plot incomes for each county in descending rank order. Using the plotly library I can make this interactive so we know which counties we’re talking about. ## Create a rank variable for income pd&lt;-pd%&gt;%mutate(percapinc_rank=rank(percapinc.2010)) ## Plot by rank gg&lt;-ggplot(data=pd , aes(x=percapinc_rank, y=percapinc.2010, text=county)) ##Add Axis Labels gg&lt;-gg+xlab(&quot;Rank&quot;)+ylab(&quot;Per Capita Income, 2010&quot;) ## Add Points gg&lt;-gg+geom_point(alpha=.5,size=.5) gg ## Save for later gg1&lt;-gg # Make Interactive plot #gg_p&lt;-ggplotly(gg) #gg_p 2.3 Unconditional Means If you were asked to predict the income for a given area without any additional information, the likely best guess is the overall average. We’re going to begin with the unconditional mean, or simple average, as our first prediction. We’ll again use the mutate command to plug in a variable that will be the average for every county, and we’ll plot this as a predictor. Our notation for the unconditional mean as a predictor is: \\[\\hat{Y}=\\bar{Y} \\] ##Unconditional Average pd%&gt;%summarize(mean_percapinc.2010=mean(percapinc.2010,na.rm=TRUE)) ## # A tibble: 1 × 1 ## mean_percapinc.2010 ## &lt;dbl&gt; ## 1 34002.98 ##Unconditional Average as a Predictor pd&lt;-pd%&gt;%mutate(mean_percapinc.2010=mean(percapinc.2010,na.rm=TRUE)) ##Plotting gg&lt;-ggplot(data=pd,aes(y=percapinc.2010,x=percapinc_rank,color=&quot;Actual&quot;)) gg&lt;-gg+geom_point(alpha=.5,size=.5) gg&lt;-gg+geom_point(aes(y=mean_percapinc.2010,x=percapinc_rank, color=&quot;Predicted: Unconditional Mean&quot;), size=.5) gg&lt;-gg+xlab(&quot;Rank of Per Capita Income&quot;)+ylab(&quot;Per Capita Income&quot;) gg&lt;-gg+scale_color_manual(name=&quot;Type&quot;, values=c(&quot;Actual&quot;=&quot;black&quot;, &quot;Predicted: Unconditional Mean&quot;=&quot;blue&quot;) ) gg&lt;-gg+theme(legend.position=&quot;bottom&quot;) gg ##Save for later gg2&lt;-gg This is of course a terrible prediction. In the absence of any other information, it’s many times the best we can do, but we really ought to be able to do better. To understand how far off we are, we need to summarize our errors. We will use different ways of doing this this semester, but let’s start with a very standard one, Root Mean Squared Error, or RMSE. An error term is the vertical distance between each point and its prediction. The RMSE is the square root of the sum of squared errors (why do we square them?). \\[RMSE(\\hat{Y})=\\sqrt{ 1/n \\sum_{i=1}^n(Y_i-\\hat{Y_i})^2} \\] pd&lt;-pd%&gt;%mutate(e1=percapinc.2010-mean_percapinc.2010) ## RMSE rmse &lt;- function(error) # Function that returns Root Mean Squared Error { sqrt(mean(error^2)) } rmse1&lt;-rmse(pd$e1) rmse1 ## [1] 7817.037 2.4 Conditional Means With One Predictor Variable To incorporate additional information into the mean, we need to calculate averages at levels of other predictors. Let’s calculate the average per capita income at different levels of college education. The code below will calculate average income across counties at four different levels of college education– the four quantiles of college education in the dataset. ##Condtional Average across a single variable myprob=.25 ## This gives the number of groups--.25=4 groups ## Create a variable for quantiles of college education pd&lt;-pd%&gt;%mutate(coll_grad_level=cut(coll_grad_pc, breaks=quantile(coll_grad_pc, probs=seq(0,1,by=myprob) ) ) ) table(pd$coll_grad_level) ## ## (3.7,13.5] (13.5,17.2] (17.2,22.9] (22.9,71.2] ## 778 772 767 770 pd&lt;-pd%&gt;%group_by(coll_grad_level)%&gt;% ## Group by predictor ##Calculate mean at each level of predictor mutate(pred_income_college=mean(percapinc.2010))%&gt;% ## Ungroup ungroup()%&gt;% #Rank by prediction, with ties sorted randomly mutate(pred_income_college_rank=rank(pred_income_college,ties.method=&quot;random&quot;))%&gt;% #Calulcate error mutate(e2=percapinc.2010-pred_income_college) gg&lt;-ggplot(data=pd,aes(x=pred_income_college_rank,y=percapinc.2010,color=&quot;Actual&quot;)) gg&lt;-gg+geom_point(alpha=.5,size=.5) gg&lt;-gg+geom_point(aes(x=pred_income_college_rank,y=pred_income_college,color=&quot;Predicted:Conditional Mean, 1 var&quot;)) gg&lt;-gg+ scale_color_manual(&quot;Type&quot;,values=c(&quot;Predicted:Conditional Mean, 1 var&quot;=&quot;red&quot;,&quot;Actual&quot;=&quot;black&quot;)) gg&lt;-gg+theme(legend.position=&quot;bottom&quot;) gg&lt;-gg+xlab(&quot;Rank&quot;)+ylab(&quot;Per Capita Income, 2010&quot;) gg ##Save for later gg3&lt;-gg Quick Exercise: Calculate per capita income as a function of the proportion of the county with a high school education Let’s see what happened to our RMSE when we did a conditional as opposed to an unconditional mean. rmse2&lt;-rmse(pd$e2) rmse2 ## [1] 6419.405 2.5 Conditional Means with Multiple Predictors The next step is to then incorporate more information from additional variables. Let’s calculate the average income by both quartiles of both the population with a bachelor’s degree and median home values. ##Condtional average across multiple variables ## Calculate quartiles of home value pd&lt;-pd%&gt;%mutate(median_home_level=cut(median_home_val, breaks=quantile(median_home_val, probs=seq(0,1,by=myprob ) ) )) table(pd$median_home_level) ## ## (1.94e+04,8.24e+04] (8.24e+04,1.07e+05] (1.07e+05,1.54e+05] ## 772 772 771 ## (1.54e+05,9.44e+05] ## 772 ##Make prediction: income by quartiles of education and home values pd&lt;-pd%&gt;%group_by(coll_grad_level,median_home_level)%&gt;% ## Grouping at multiple levels mutate(pred_income_college_home=mean(percapinc.2010))%&gt;%ungroup()%&gt;% mutate(pred_income_college_home_rank=rank(pred_income_college_home, ties.method=&quot;random&quot;))%&gt;% mutate(e3=percapinc.2010-pred_income_college_home) ## Showing the various levels quantile(x=pd$pred_income_college_home,probs=seq(0,1,by=myprob)) ## 0% 25% 50% 75% 100% ## 21290.00 29725.54 33376.52 36923.41 42015.80 gg&lt;-ggplot(data=pd,aes(x=pred_income_college_home_rank,y=percapinc.2010,color=&quot;Actual&quot;)) gg&lt;-gg+geom_point(alpha=.5,size=.5) gg&lt;-gg+geom_point(aes(x=pred_income_college_home_rank, y=pred_income_college_home, color=&quot;Predicted:Conditional Mean, 2 vars&quot;) ) gg&lt;-gg+scale_color_manual(&quot;Type&quot;,values=c(&quot;Actual&quot;=&quot;black&quot;, &quot;Predicted:Conditional Mean, 2 vars&quot;=&quot;orange&quot; )) gg&lt;-gg+theme(legend.position=&quot;bottom&quot;) gg&lt;-gg+xlab(&quot;Rank&quot;)+ylab(&quot;Per Capita Income&quot;) gg ## Save for later gg4&lt;-gg This is clearly much better: our predictions appear to be much closer to the actual data points more of the time. And what happened to RMSE? rmse3&lt;-rmse(pd$e3) rmse3 ## [1] 6240.328 2.6 Final Comparison Let’s put all of the plots together to compare. gg5&lt;-grid.arrange(gg1,gg2,gg3,gg4,nrow=2) gg5 ## TableGrob (2 x 2) &quot;arrange&quot;: 4 grobs ## z cells name grob ## 1 1 (1-1,1-1) arrange gtable[layout] ## 2 2 (1-1,2-2) arrange gtable[layout] ## 3 3 (2-2,1-1) arrange gtable[layout] ## 4 4 (2-2,2-2) arrange gtable[layout] Quick Exercise: Predict income using two other variables 2.7 Applications of the Conditional Mean When might we use the conditional mean? Caluclating average sales for a retail location by day of the week and month Calculating yield rate (proportion of admitted students who attend) by geographic region and income level for a college. Calculating average employee turnover by level of education and gender "],
["plot-means.html", "3 Presenting Data 1: Plotting Conditional Means 3.1 Setup for plotting conditional means 3.2 Loading Data 3.3 Conditional means using two predictors 3.4 Multiple Predictors for Conditional Means 3.5 Univariate Graphics", " 3 Presenting Data 1: Plotting Conditional Means The idea when plotting conditional means is to show how the outcome, or variable of interest, varies as a function of predictors. Today we’ll be working with a dataset from IBM which provide a standard HR dataset, which we can use to predict attrition. Attrition in this case is defined as an employee leaving without being fired or retiring. Companies generally attempt to avoid attrition, as it’s very expensive to search for and hire a replacement– better in general to keep the employees you have, provided they are doing their jobs. This means that it’s important to predict who might leave in a given year. This information can be used in a targeted way in order to focus resources on the employees most likely to leave. 3.1 Setup for plotting conditional means We start with a standard set of setup commands. Today we’ll be working with dplyr, ggplot plus readr and forcats. Next we load in the data, using the readr package. Note that this data is saved as comma separated or csv. This is an easy file format to recognize. When we use the readr package, it gives us some output that says how it interprets the data– is it a string variable, numeric (float), integer and so on. 3.2 Loading Data at&lt;-read_csv(&quot;https://community.watsonanalytics.com/wp-content/uploads/2015/03/WA_Fn-UseC_-HR-Employee-Attrition.csv&quot;) ## Parsed with column specification: ## cols( ## .default = col_integer(), ## Attrition = col_character(), ## BusinessTravel = col_character(), ## Department = col_character(), ## EducationField = col_character(), ## Gender = col_character(), ## JobRole = col_character(), ## MaritalStatus = col_character(), ## Over18 = col_character(), ## OverTime = col_character() ## ) ## See spec(...) for full column specifications. ## Save for later save(at,file=&quot;at.Rdata&quot;) ## load(&quot;at.Rdata&quot;) Today, our primary outcome of interest will be attrition. This is a binary variable that is currently encoded as text– “Yes” or “No.” We need to encode it as a binary variable with 1 meaning yes and 0 meaning no. After recoding, we need to make sure that the new variable looks correct. ## Crate a new variable named attrit and define it as 0 at$attrit&lt;-0 at$attrit[at$Attrition==&quot;Yes&quot;]&lt;-1 table(at$Attrition) ## ## No Yes ## 1233 237 table(at$attrit) ## ## 0 1 ## 1233 237 table(at$attrit,at$Attrition) ## ## No Yes ## 0 1233 0 ## 1 0 237 Our first prediction will use business travel as a predictor for attrition. There are three categories here– non travel, travel infrequently, and frequent travel. We’ll calculate levels of attrtion at teach level and then take a look at the data. at_sum&lt;-at%&gt;% group_by(BusinessTravel)%&gt;% summarize(attr_avg=mean(attrit)) at_sum ## # A tibble: 3 × 2 ## BusinessTravel attr_avg ## &lt;chr&gt; &lt;dbl&gt; ## 1 Non-Travel 0.0800000 ## 2 Travel_Frequently 0.2490975 ## 3 Travel_Rarely 0.1495686 Remember that the mean of a binary variable indicates the proportion of the population that has a certain characteristcs. So, in our case, 0.2490975 of the sample that travels frequently left the compoany in the last year. Our first plot will be a basic bar plot, showing the average levels of attrition. ## Bar Plot with aesthentics: mean attrition as height, business travel as cateogry gg&lt;-ggplot(at_sum,aes(x=BusinessTravel,y=attr_avg)) ## Use bar plot geometry, height of bars set by level observed in dataset gg&lt;-gg+geom_bar(stat=&quot;Identity&quot;) ## Print gg This is fine, but it should really be in the order of the underlying variable. We can use fct_reorder to do this. ## Same asethetics, but now orderred by level gg&lt;-ggplot(at_sum,aes(x=fct_reorder(BusinessTravel,attr_avg),y=attr_avg)) gg&lt;-gg+geom_bar(stat=&quot;identity&quot;) ##Print gg at_sum&lt;-at%&gt;% group_by(Department)%&gt;% summarize(attr_avg=mean(attrit)) at_sum ## # A tibble: 3 × 2 ## Department attr_avg ## &lt;chr&gt; &lt;dbl&gt; ## 1 Human Resources 0.1904762 ## 2 Research &amp; Development 0.1383975 ## 3 Sales 0.2062780 gg&lt;-ggplot(at_sum,aes(x=fct_reorder(Department,attr_avg),y=attr_avg)) gg&lt;-gg+geom_bar(stat=&quot;identity&quot;) ##Print gg Quick Exercise: Create a bar plot showing average attrition by department instead of travel A dot plot can be a good way of displaying conditional means as well. Many times dot plots are more easily understood if they are horizontal, so we’ll use coord_flip to make it horizontal. at_sum&lt;-at%&gt;% group_by(BusinessTravel)%&gt;% summarize(attr_avg=mean(attrit)) at_sum ## # A tibble: 3 × 2 ## BusinessTravel attr_avg ## &lt;chr&gt; &lt;dbl&gt; ## 1 Non-Travel 0.0800000 ## 2 Travel_Frequently 0.2490975 ## 3 Travel_Rarely 0.1495686 ## Now a dot plot gg&lt;-ggplot(at_sum,aes(x=reorder(BusinessTravel,attr_avg),y=attr_avg)) gg&lt;-gg+geom_point() gg&lt;-gg+coord_flip() gg Quick Exercise: Create a dot plot showing average attrition by department 3.3 Conditional means using two predictors We can use graphics to display conditonal means at multiple levels of predictor levels. There are a couple of ways to get this done. When using bar plots we’ve got two basic tools: location and color. In the first example, we’re going to plot attrition by travel and gender, We’ll use color to indicate gender, and location to indicate travel. at_sum&lt;-at%&gt;% group_by(BusinessTravel,Gender)%&gt;% summarize(attr_avg=mean(attrit)) at_sum ## Source: local data frame [6 x 3] ## Groups: BusinessTravel [?] ## ## BusinessTravel Gender attr_avg ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Non-Travel Female 0.06122449 ## 2 Non-Travel Male 0.08910891 ## 3 Travel_Frequently Female 0.25641026 ## 4 Travel_Frequently Male 0.24375000 ## 5 Travel_Rarely Female 0.12796209 ## 6 Travel_Rarely Male 0.16425121 gg&lt;-ggplot(at_sum,aes(x=reorder(BusinessTravel,attr_avg),y=attr_avg,color=Gender)) gg&lt;-gg+geom_bar(stat=&quot;identity&quot;,aes(fill=Gender),position=&quot;dodge&quot;) gg gg&lt;-ggplot(at_sum,aes(x=reorder(BusinessTravel,attr_avg),y=attr_avg),color=Gender) gg&lt;-gg+geom_point(aes(color=Gender)) gg&lt;-gg+coord_flip() gg at_sum&lt;-at%&gt;% group_by(Department,EducationField)%&gt;% summarize(attr_avg=mean(attrit)) at_sum ## Source: local data frame [14 x 3] ## Groups: Department [?] ## ## Department EducationField attr_avg ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Human Resources Human Resources 0.2592593 ## 2 Human Resources Life Sciences 0.0625000 ## 3 Human Resources Medical 0.1538462 ## 4 Human Resources Other 0.0000000 ## 5 Human Resources Technical Degree 0.5000000 ## 6 Research &amp; Development Life Sciences 0.1340909 ## 7 Research &amp; Development Medical 0.1294766 ## 8 Research &amp; Development Other 0.1093750 ## 9 Research &amp; Development Technical Degree 0.2127660 ## 10 Sales Life Sciences 0.1933333 ## 11 Sales Marketing 0.2201258 ## 12 Sales Medical 0.1590909 ## 13 Sales Other 0.2666667 ## 14 Sales Technical Degree 0.2941176 gg&lt;-ggplot(at_sum,aes(x=reorder(EducationField,attr_avg),y=attr_avg,color=Department)) gg&lt;-gg+geom_bar(stat=&quot;identity&quot;,aes(fill=Department), position=&quot;dodge&quot;) gg Quick Exercise: Create either a bar plot or a dot plot showing attrition by department AND field of education at_sum&lt;-at%&gt;% group_by(BusinessTravel,Gender,MaritalStatus)%&gt;% summarize(attr_avg=mean(attrit)) at_sum ## Source: local data frame [18 x 4] ## Groups: BusinessTravel, Gender [?] ## ## BusinessTravel Gender MaritalStatus attr_avg ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Non-Travel Female Divorced 0.00000000 ## 2 Non-Travel Female Married 0.00000000 ## 3 Non-Travel Female Single 0.20000000 ## 4 Non-Travel Male Divorced 0.03030303 ## 5 Non-Travel Male Married 0.08333333 ## 6 Non-Travel Male Single 0.15625000 ## 7 Travel_Frequently Female Divorced 0.20833333 ## 8 Travel_Frequently Female Married 0.13636364 ## 9 Travel_Frequently Female Single 0.38775510 ## 10 Travel_Frequently Male Divorced 0.20512821 ## 11 Travel_Frequently Male Married 0.17567568 ## 12 Travel_Frequently Male Single 0.38297872 ## 13 Travel_Rarely Female Divorced 0.04878049 ## 14 Travel_Rarely Female Married 0.12195122 ## 15 Travel_Rarely Female Single 0.18518519 ## 16 Travel_Rarely Male Divorced 0.10869565 ## 17 Travel_Rarely Male Married 0.12714777 ## 18 Travel_Rarely Male Single 0.26041667 gg&lt;-ggplot(at_sum,aes(x=reorder(BusinessTravel,attr_avg), y=attr_avg, color=Gender)) ## Bar plot, with unstacked (dodge) gg&lt;-gg+geom_bar(aes(fill=Gender),stat=&quot;identity&quot;,position=&quot;dodge&quot;) ## Separate out by Marital Status gg&lt;-gg+facet_wrap(~MaritalStatus) ## Change orientation to sideways gg&lt;-gg+coord_flip() ## Print gg gg&lt;-ggplot(at_sum,aes(x=reorder(BusinessTravel,attr_avg), y=attr_avg)) gg&lt;-gg+geom_point(aes(color=MaritalStatus,shape=Gender)) gg at_sum&lt;-at%&gt;% group_by(EducationField,Gender,Department)%&gt;% summarize(attr_avg=mean(attrit)) at_sum ## Source: local data frame [27 x 4] ## Groups: EducationField, Gender [?] ## ## EducationField Gender Department attr_avg ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Human Resources Female Human Resources 0.3750000 ## 2 Human Resources Male Human Resources 0.2105263 ## 3 Life Sciences Female Human Resources 0.1111111 ## 4 Life Sciences Female Research &amp; Development 0.1445783 ## 5 Life Sciences Female Sales 0.2000000 ## 6 Life Sciences Male Human Resources 0.0000000 ## 7 Life Sciences Male Research &amp; Development 0.1277372 ## 8 Life Sciences Male Sales 0.1882353 ## 9 Marketing Female Sales 0.2173913 ## 10 Marketing Male Sales 0.2222222 ## # ... with 17 more rows gg&lt;-ggplot(at_sum,aes(x=reorder(Department,attr_avg), y=attr_avg, color=Gender)) ## Bar plot, with unstacked (dodge) gg&lt;-gg+geom_bar(aes(fill=Gender),stat=&quot;identity&quot;,position=&quot;dodge&quot;,color=&quot;black&quot;) ## Separate out by Marital Status gg&lt;-gg+facet_wrap(~EducationField) ## Change orientation to sideways gg&lt;-gg+coord_flip() ## Changing Colors mypal&lt;-c(&quot;lightblue&quot;,&quot;darkgoldenrod&quot;) gg&lt;-gg+scale_fill_manual(values =mypal ) ## Print gg ## Another way gg&lt;-gg+scale_fill_brewer(palette = &quot;YlOrRd&quot;) ## Scale for &#39;fill&#39; is already present. Adding another scale for &#39;fill&#39;, ## which will replace the existing scale. gg Quick Exercise: Plot predicted attrition by Education Field, Department and Gender 3.4 Multiple Predictors for Conditional Means Once you get past three variables, things can get difficult. One solution is to create a new factor with one level for every single level of the predictor variables. ##This gets a little nutty at_sum&lt;-at%&gt;% group_by(BusinessTravel,Gender,MaritalStatus,WorkLifeBalance)%&gt;% summarize(attr_avg=mean(attrit))%&gt;% ungroup()%&gt;% arrange(attr_avg) at_sum ## # A tibble: 70 × 5 ## BusinessTravel Gender MaritalStatus WorkLifeBalance attr_avg ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Non-Travel Female Divorced 2 0 ## 2 Non-Travel Female Divorced 3 0 ## 3 Non-Travel Female Divorced 4 0 ## 4 Non-Travel Female Married 1 0 ## 5 Non-Travel Female Married 2 0 ## 6 Non-Travel Female Married 3 0 ## 7 Non-Travel Female Married 4 0 ## 8 Non-Travel Female Single 1 0 ## 9 Non-Travel Female Single 3 0 ## 10 Non-Travel Male Divorced 2 0 ## # ... with 60 more rows ## One Solution at_sum$grouping&lt;-paste0(at_sum$BusinessTravel, &quot;, &quot;, at_sum$Gender, &quot;, &quot;, at_sum$MaritalStatus, &quot;, Work/Life:&quot;, at_sum$WorkLifeBalance) at_sum$grouping&lt;-as.factor(at_sum$grouping) at_sum%&gt;%select(grouping,attr_avg) ## # A tibble: 70 × 2 ## grouping attr_avg ## &lt;fctr&gt; &lt;dbl&gt; ## 1 Non-Travel, Female, Divorced, Work/Life:2 0 ## 2 Non-Travel, Female, Divorced, Work/Life:3 0 ## 3 Non-Travel, Female, Divorced, Work/Life:4 0 ## 4 Non-Travel, Female, Married, Work/Life:1 0 ## 5 Non-Travel, Female, Married, Work/Life:2 0 ## 6 Non-Travel, Female, Married, Work/Life:3 0 ## 7 Non-Travel, Female, Married, Work/Life:4 0 ## 8 Non-Travel, Female, Single, Work/Life:1 0 ## 9 Non-Travel, Female, Single, Work/Life:3 0 ## 10 Non-Travel, Male, Divorced, Work/Life:2 0 ## # ... with 60 more rows at_sum&lt;-at_sum%&gt;%filter(attr_avg&gt;.01) gg&lt;-ggplot(at_sum,aes(x=fct_reorder(grouping,attr_avg),y=attr_avg)) gg&lt;-gg+geom_bar(stat=&quot;identity&quot;,aes(fill=MaritalStatus)) gg&lt;-gg+coord_flip() gg ##Cleaning up a bit gg&lt;-ggplot(at_sum,aes(x=fct_reorder(grouping,attr_avg),y=attr_avg)) gg&lt;-gg+geom_bar(stat=&quot;identity&quot;,aes(fill=MaritalStatus)) gg&lt;-gg+ylab(&quot;Proportion of Employees Who Departed&quot;)+xlab(&quot;Category&quot;) gg&lt;-gg+coord_flip() gg The other solution is to use facets, or lots of little graphs, which show how the pattern varies across different groups. In this case, our groups will be defined by gender and work/life balance. ## Using Facets at_sum&lt;-at%&gt;% group_by(BusinessTravel,Gender,MaritalStatus,WorkLifeBalance)%&gt;% summarize(attr_avg=mean(attrit))%&gt;% ungroup()%&gt;% arrange(attr_avg) at_sum ## # A tibble: 70 × 5 ## BusinessTravel Gender MaritalStatus WorkLifeBalance attr_avg ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Non-Travel Female Divorced 2 0 ## 2 Non-Travel Female Divorced 3 0 ## 3 Non-Travel Female Divorced 4 0 ## 4 Non-Travel Female Married 1 0 ## 5 Non-Travel Female Married 2 0 ## 6 Non-Travel Female Married 3 0 ## 7 Non-Travel Female Married 4 0 ## 8 Non-Travel Female Single 1 0 ## 9 Non-Travel Female Single 3 0 ## 10 Non-Travel Male Divorced 2 0 ## # ... with 60 more rows gg&lt;-ggplot(at_sum,aes(x=fct_reorder(BusinessTravel,attr_avg),y=attr_avg)) gg&lt;-gg+geom_bar(stat=&quot;identity&quot;,aes(fill=MaritalStatus),position=&quot;dodge&quot;) gg&lt;-gg+facet_wrap(~Gender+WorkLifeBalance,ncol=4) gg&lt;-gg+ylab(&quot;Proportion of Employees Who Departed&quot;)+xlab(&quot;Category&quot;) gg&lt;-gg+theme(axis.text.x = element_text(angle = 60, hjust = 1)) gg&lt;-gg+ggtitle(&quot;Departure by Gender and Level of Work/Life Satisfaction&quot;) gg Sort of Quick Exercise: Try and Replicate one of the above plots using performance review, department, education field and overtime. 3.5 Univariate Graphics Here’s a quick rundown on some univariate graphics. Say we wanted a quick count of who was in each department. We can use geom_bar to get this done. By default, this will give us a count in each department. gg&lt;-ggplot(at,aes(x=Department,fill=Department)) gg&lt;-gg+geom_bar() mypal&lt;-c(&quot;lightblue&quot;,&quot;yellow3&quot;,&quot;darkorchid1&quot;) gg&lt;-gg+scale_fill_manual(values=mypal) gg The next univariate graphic you should know is for continuous variables. The first thing you generally want is a histogram. gg&lt;-ggplot(at,aes(x=DistanceFromHome)) gg&lt;-gg+geom_histogram(binwidth = 1,fill=&quot;lightblue&quot;) gg Density plots provide a continous graphic of the distribution of a variable: gg&lt;-ggplot(at,aes(x=DistanceFromHome)) gg&lt;-gg+geom_density() gg gg&lt;-gg+geom_density(bw=.2) gg "],
["flat-data.html", "4 Working with flat data files 4.1 CSV or other delimited files 4.2 Fixed width files 4.3 Excel (sigh)", " 4 Working with flat data files Flat data is data that is arranged with one case per row, with one column per variable– more or less. It’s stored in a variety of formats, with different conventions. Our goal is to get it into the most useful format for analysis: what’s known as tidy data. library(tidyverse) library(haven) library(readxl) 4.1 CSV or other delimited files We’ll start with a csv file which is among the most common types. CSV stands for _Comma _Separated _Value, meaning that each row is divided into cells by commas. An end of line completes the row. #Delimited files #Load in the HSB dataset from the UCLA statistical computing site hsb&lt;-read_csv(file=&quot;https://stats.idre.ucla.edu/wp-content/uploads/2016/02/hsb2-2.csv&quot;) ## Parsed with column specification: ## cols( ## id = col_integer(), ## female = col_integer(), ## race = col_integer(), ## ses = col_integer(), ## schtyp = col_integer(), ## prog = col_integer(), ## read = col_integer(), ## write = col_integer(), ## math = col_integer(), ## science = col_integer(), ## socst = col_integer() ## ) write_csv(hsb,path=&quot;hsb.csv&quot;) #Check it out head(hsb) ## # A tibble: 6 × 11 ## id female race ses schtyp prog read write math science socst ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 70 0 4 1 1 1 57 52 41 47 57 ## 2 121 1 4 2 1 3 68 59 53 63 61 ## 3 86 0 4 3 1 1 44 33 54 58 31 ## 4 141 0 4 3 1 3 63 44 47 53 56 ## 5 172 0 4 2 1 2 47 52 57 53 61 ## 6 113 0 4 2 1 2 44 52 51 63 61 ##Need these for later my.names&lt;-names(hsb) #Write this in a variety of formats to be used later write_delim(hsb, path=&quot;hsb.txt&quot;,delim=&quot;\\t&quot;) write_delim(hsb, path=&quot;hsb_semicolon.txt&quot;,delim=&quot;;&quot;) gdata::write.fwf(data.frame(hsb),file=&quot;hsb.dat&quot;,sep=&quot;&quot;,colnames=FALSE) Quick exercise: write out the HSB file with a semicolon delimiter 4.2 Fixed width files fixed width files are an older file format that you don’t see as much of any more. To read these in, you need a file that tells you the locations of the different variables, known as column positions or locations. ## ------------------------------------------------------------------------ #Fixed width files #You need to get the &quot;widths&quot; somehere, usually a data dictionary my.widths=c(3,#id 1, #female 1, #race 1, #ses 1, #schtyp 1, #prog 2, #read 2, #write 2, #math 2, #science 2 #socst ) my_positions&lt;-fwf_widths(my.widths) hsb3&lt;-read_fwf(&quot;hsb.dat&quot;, col_positions =my_positions) ## Parsed with column specification: ## cols( ## X1 = col_integer(), ## X2 = col_integer(), ## X3 = col_integer(), ## X4 = col_integer(), ## X5 = col_integer(), ## X6 = col_integer(), ## X7 = col_integer(), ## X8 = col_integer(), ## X9 = col_integer(), ## X10 = col_integer(), ## X11 = col_integer() ## ) head(hsb3) ## # A tibble: 6 × 11 ## X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 70 0 4 1 1 1 57 52 41 47 57 ## 2 121 1 4 2 1 3 68 59 53 63 61 ## 3 86 0 4 3 1 1 44 33 54 58 31 ## 4 141 0 4 3 1 3 63 44 47 53 56 ## 5 172 0 4 2 1 2 47 52 57 53 61 ## 6 113 0 4 2 1 2 44 52 51 63 61 names(hsb3)&lt;-my.names head(hsb3) ## # A tibble: 6 × 11 ## id female race ses schtyp prog read write math science socst ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 70 0 4 1 1 1 57 52 41 47 57 ## 2 121 1 4 2 1 3 68 59 53 63 61 ## 3 86 0 4 3 1 1 44 33 54 58 31 ## 4 141 0 4 3 1 3 63 44 47 53 56 ## 5 172 0 4 2 1 2 47 52 57 53 61 ## 6 113 0 4 2 1 2 44 52 51 63 61 4.3 Excel (sigh) In most business applications you’ll work with excel files the most often. To get these into shape you’ll have to do some wrangling. Below I show how this is done with data in a common reporting format. ## ------------------------------------------------------------------------ #Excel files ##http://nces.ed.gov/programs/digest/d14/tables/dt14_204.10.asp download.file(&quot;http://nces.ed.gov/programs/digest/d14/tables/xls/tabn204.10.xls&quot;,destfile=&quot;free.xls&quot;) free&lt;-read_excel(&quot;free.xls&quot;,skip=4,col_names=FALSE) ## DEFINEDNAME: 00 00 00 0f 02 00 00 00 00 00 00 00 00 00 00 5f 34 57 4f 52 44 5f 4d 5f 30 30 31 5f 30 37 1c 2a ## DEFINEDNAME: 00 00 00 0f 02 00 00 00 00 00 00 00 00 00 00 5f 34 57 4f 52 44 5f 4f 5f 30 30 35 5f 4c 5f 1c 2a ## DEFINEDNAME: 01 00 00 0f 03 00 00 00 01 00 00 00 00 00 00 5f 52 65 67 72 65 73 73 69 6f 6e 5f 49 6e 74 1e 01 00 ## DEFINEDNAME: 20 00 00 01 0b 00 00 00 01 00 00 00 00 00 00 06 3b 00 00 00 00 44 00 00 00 12 00 ## DEFINEDNAME: 00 00 00 0d 0b 00 00 00 01 00 00 00 00 00 00 50 72 69 6e 74 5f 41 72 65 61 5f 4d 49 3b 00 00 00 00 46 00 00 00 0d 00 ## DEFINEDNAME: 00 00 00 0f 02 00 00 00 00 00 00 00 00 00 00 5f 34 57 4f 52 44 5f 4d 5f 30 30 31 5f 30 37 1c 2a ## DEFINEDNAME: 00 00 00 0f 02 00 00 00 00 00 00 00 00 00 00 5f 34 57 4f 52 44 5f 4f 5f 30 30 35 5f 4c 5f 1c 2a ## DEFINEDNAME: 01 00 00 0f 03 00 00 00 01 00 00 00 00 00 00 5f 52 65 67 72 65 73 73 69 6f 6e 5f 49 6e 74 1e 01 00 ## DEFINEDNAME: 20 00 00 01 0b 00 00 00 01 00 00 00 00 00 00 06 3b 00 00 00 00 44 00 00 00 12 00 ## DEFINEDNAME: 00 00 00 0d 0b 00 00 00 01 00 00 00 00 00 00 50 72 69 6e 74 5f 41 72 65 61 5f 4d 49 3b 00 00 00 00 46 00 00 00 0d 00 ## DEFINEDNAME: 00 00 00 0f 02 00 00 00 00 00 00 00 00 00 00 5f 34 57 4f 52 44 5f 4d 5f 30 30 31 5f 30 37 1c 2a ## DEFINEDNAME: 00 00 00 0f 02 00 00 00 00 00 00 00 00 00 00 5f 34 57 4f 52 44 5f 4f 5f 30 30 35 5f 4c 5f 1c 2a ## DEFINEDNAME: 01 00 00 0f 03 00 00 00 01 00 00 00 00 00 00 5f 52 65 67 72 65 73 73 69 6f 6e 5f 49 6e 74 1e 01 00 ## DEFINEDNAME: 20 00 00 01 0b 00 00 00 01 00 00 00 00 00 00 06 3b 00 00 00 00 44 00 00 00 12 00 ## DEFINEDNAME: 00 00 00 0d 0b 00 00 00 01 00 00 00 00 00 00 50 72 69 6e 74 5f 41 72 65 61 5f 4d 49 3b 00 00 00 00 46 00 00 00 0d 00 head(free) ## # A tibble: 6 × 19 ## X1 X2 X3 X4 X5 X6 ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 United States ............. 46579068 \\\\1\\\\ 48941267 48995812 \\\\1\\\\ ## 2 Alabama .................... 728351 &lt;NA&gt; 730427 731556 &lt;NA&gt; ## 3 Alaska .................. 105333 &lt;NA&gt; 132104 131166 &lt;NA&gt; ## 4 Arizona ..................... 877696 \\\\2\\\\ 1067210 1024454 &lt;NA&gt; ## 5 Arkansas .................. 449959 &lt;NA&gt; 482114 483114 &lt;NA&gt; ## 6 California ................. 6050753 &lt;NA&gt; 6169427 6202862 \\\\2\\\\ ## # ... with 13 more variables: X7 &lt;dbl&gt;, X8 &lt;dbl&gt;, X9 &lt;chr&gt;, X10 &lt;dbl&gt;, ## # X11 &lt;dbl&gt;, X12 &lt;chr&gt;, X13 &lt;dbl&gt;, X14 &lt;dbl&gt;, X15 &lt;chr&gt;, X16 &lt;dbl&gt;, ## # X17 &lt;dbl&gt;, X18 &lt;chr&gt;, X19 &lt;dbl&gt; # Now need to clean up #Get rid of unwanted columns free2&lt;-free[ ,-(c(3,6,9,12,15,18))] #Get rid of unwanted rows free2&lt;-free2%&gt;%filter(is.na(X1)==FALSE) ##50 states plus dc only free2&lt;-free2[2:52,] head(free2) ## # A tibble: 6 × 13 ## X1 X2 X4 X5 X7 X8 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alabama .................... 728351 730427 731556 740475 335143 ## 2 Alaska .................. 105333 132104 131166 131483 32468 ## 3 Arizona ..................... 877696 1067210 1024454 990378 274277 ## 4 Arkansas .................. 449959 482114 483114 486157 205058 ## 5 California ................. 6050753 6169427 6202862 6178788 2820611 ## 6 Colorado .................... 724349 842864 853610 863121 195148 ## # ... with 7 more variables: X10 &lt;dbl&gt;, X11 &lt;dbl&gt;, X13 &lt;dbl&gt;, X14 &lt;dbl&gt;, ## # X16 &lt;dbl&gt;, X17 &lt;dbl&gt;, X19 &lt;dbl&gt; tail(free2) ## # A tibble: 6 × 13 ## X1 X2 X4 X5 X7 X8 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Vermont .................... 102049 85144 83451 83568 23986 ## 2 Virginia .................... 1067710 1250206 1227099 1235561 320233 ## 3 Washington .................. 1004770 1043466 1041934 1050904 326295 ## 4 West Virginia ........... 286285 282879 282870 283044 143446 ## 5 Wisconsin ................ 859276 872164 869670 871376 219276 ## 6 Wyoming ................... 89895 88779 89363 91533 43483 ## # ... with 7 more variables: X10 &lt;dbl&gt;, X11 &lt;dbl&gt;, X13 &lt;dbl&gt;, X14 &lt;dbl&gt;, ## # X16 &lt;dbl&gt;, X17 &lt;dbl&gt;, X19 &lt;dbl&gt; names(free2)&lt;-c(&quot;state&quot;, &quot;total.2000&quot;, &quot;total.2010&quot;, &quot;total.2011&quot;, &quot;total.2012&quot;, &quot;frl.2000&quot;, &quot;frl.2010&quot;, &quot;frl.2011&quot;, &quot;frl.2012&quot;, &quot;pc.frl.2000&quot;, &quot;pc.frl.2010&quot;, &quot;pc.frl.2011&quot;, &quot;pc.frl.2012&quot;) *Quick Exercise: Read in this file: http://nces.ed.gov/programs/digest/d14/tables/xls/tabn302.10.xls* download.file(&quot;http://nces.ed.gov/programs/digest/d14/tables/xls/tabn302.10.xls&quot;,destfile=&quot;grad.xls&quot;) grad&lt;-read_excel(&quot;grad.xls&quot;,skip=6,col_names=FALSE) ## DEFINEDNAME: 01 00 00 0f 03 00 00 00 01 00 00 00 00 00 00 5f 52 65 67 72 65 73 73 69 6f 6e 5f 49 6e 74 1e 01 00 ## DEFINEDNAME: 20 00 00 01 0b 00 00 00 01 00 00 00 00 00 00 06 3b 00 00 00 00 4c 00 00 00 18 00 ## DEFINEDNAME: 20 00 00 01 02 00 00 00 00 00 00 00 00 00 00 06 1c 2a ## DEFINEDNAME: 00 00 00 0d 0b 00 00 00 01 00 00 00 00 00 00 50 72 69 6e 74 5f 41 72 65 61 5f 4d 49 3b 00 00 00 00 4c 00 00 00 14 00 ## DEFINEDNAME: 00 00 00 0d 02 00 00 00 00 00 00 00 00 00 00 50 52 49 4e 54 5f 41 52 45 41 5f 4d 49 1c 2a ## DEFINEDNAME: 01 00 00 0f 03 00 00 00 01 00 00 00 00 00 00 5f 52 65 67 72 65 73 73 69 6f 6e 5f 49 6e 74 1e 01 00 ## DEFINEDNAME: 20 00 00 01 0b 00 00 00 01 00 00 00 00 00 00 06 3b 00 00 00 00 4c 00 00 00 18 00 ## DEFINEDNAME: 20 00 00 01 02 00 00 00 00 00 00 00 00 00 00 06 1c 2a ## DEFINEDNAME: 00 00 00 0d 0b 00 00 00 01 00 00 00 00 00 00 50 72 69 6e 74 5f 41 72 65 61 5f 4d 49 3b 00 00 00 00 4c 00 00 00 14 00 ## DEFINEDNAME: 00 00 00 0d 02 00 00 00 00 00 00 00 00 00 00 50 52 49 4e 54 5f 41 52 45 41 5f 4d 49 1c 2a ## DEFINEDNAME: 01 00 00 0f 03 00 00 00 01 00 00 00 00 00 00 5f 52 65 67 72 65 73 73 69 6f 6e 5f 49 6e 74 1e 01 00 ## DEFINEDNAME: 20 00 00 01 0b 00 00 00 01 00 00 00 00 00 00 06 3b 00 00 00 00 4c 00 00 00 18 00 ## DEFINEDNAME: 20 00 00 01 02 00 00 00 00 00 00 00 00 00 00 06 1c 2a ## DEFINEDNAME: 00 00 00 0d 0b 00 00 00 01 00 00 00 00 00 00 50 72 69 6e 74 5f 41 72 65 61 5f 4d 49 3b 00 00 00 00 4c 00 00 00 14 00 ## DEFINEDNAME: 00 00 00 0d 02 00 00 00 00 00 00 00 00 00 00 50 52 49 4e 54 5f 41 52 45 41 5f 4d 49 1c 2a #Get rid of unwanted rows grad&lt;-grad%&gt;%filter(is.na(X1)==FALSE) "],
["tidy-data.html", "5 Tidy data 5.1 Other programming languages 5.2 Output", " 5 Tidy data Tidy data follows two key principles: each column is one variable and one variable only, while each row is a case. Below, I show how to make the data from the above spreadsheet tidy, and why we would do this. free_total&lt;-free2%&gt;%select(state, total.2000, total.2010, total.2011, total.2012) names(free_total)&lt;-c(&quot;state&quot;,&quot;2000&quot;,&quot;2010&quot;,&quot;2011&quot;,&quot;2012&quot;) free_total&lt;-free_total%&gt;%gather(`2000`,`2010`,`2011`,`2012`,key=year,value=total_students) frl_total&lt;-free2%&gt;%select(state, frl.2000, frl.2010, frl.2011, frl.2012) names(frl_total)&lt;-c(&quot;state&quot;,&quot;2000&quot;,&quot;2010&quot;,&quot;2011&quot;,&quot;2012&quot;) frl_total&lt;-frl_total%&gt;%gather(`2000`,`2010`,`2011`,`2012`,key=year,value=frl_students) free_tidy&lt;-left_join(free_total,frl_total,by=c(&quot;state&quot;,&quot;year&quot;)) free_tidy ## # A tibble: 204 × 4 ## state year total_students frl_students ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alabama .................... 2000 728351 335143 ## 2 Alaska .................. 2000 105333 32468 ## 3 Arizona ..................... 2000 877696 274277 ## 4 Arkansas .................. 2000 449959 205058 ## 5 California ................. 2000 6050753 2820611 ## 6 Colorado .................... 2000 724349 195148 ## 7 Connecticut ............... 2000 562179 143030 ## 8 Delaware .................. 2000 114676 37766 ## 9 District of Columbia ........ 2000 68380 47839 ## 10 Florida .................. 2000 2434755 1079009 ## # ... with 194 more rows ## Total by year free_tidy%&gt;%group_by(year)%&gt;%summarize(sum(frl_students)) ## # A tibble: 4 × 2 ## year `sum(frl_students)` ## &lt;chr&gt; &lt;dbl&gt; ## 1 2000 17839867 ## 2 2010 23544479 ## 3 2011 24291646 ## 4 2012 25188294 pc.frl_total&lt;-free2%&gt;%select(state, pc.frl.2000, pc.frl.2010, pc.frl.2011, pc.frl.2012) names(pc.frl_total)&lt;-c(&quot;state&quot;,&quot;2000&quot;,&quot;2010&quot;,&quot;2011&quot;,&quot;2012&quot;) pc_frl_total&lt;-pc.frl_total%&gt;%gather(`2000`,`2010`,`2011`,`2012`,key=year,value=pc_frl_students) ## ------------------------------------------------------------------------ Quick Exericse: now add in percent of students eligible by state 5.1 Other programming languages Other statistical programs have their own file formats. These are easy for these programs to read in. R can udnerstand all of them, if the haven packages is used. #Other data files from stat programming language # Stata hsb_stata&lt;-read_dta(&quot;https://stats.idre.ucla.edu/stat/stata/notes/hsb2.dta&quot;) head(hsb_stata) ## # A tibble: 6 × 11 ## id female race ses schtyp prog read write ## &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 70 0 4 1 1 1 57 52 ## 2 121 1 4 2 1 3 68 59 ## 3 86 0 4 3 1 1 44 33 ## 4 141 0 4 3 1 3 63 44 ## 5 172 0 4 2 1 2 47 52 ## 6 113 0 4 2 1 2 44 52 ## # ... with 3 more variables: math &lt;dbl&gt;, science &lt;dbl&gt;, socst &lt;dbl&gt; #SPSS example_spss&lt;-read_spss(&quot;https://stats.idre.ucla.edu/stat/data/binary.sav&quot;) head(example_spss) ## # A tibble: 6 × 4 ## admit gre gpa rank ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 380 3.61 3 ## 2 1 660 3.67 3 ## 3 1 800 4.00 1 ## 4 1 640 3.19 4 ## 5 0 520 2.93 4 ## 6 1 760 3.00 2 #SAS hsb_sas&lt;-read_sas(&quot;https://stats.idre.ucla.edu/wp-content/uploads/2016/02/hsb2.sas7bdat&quot;) head(hsb_sas) ## # A tibble: 6 × 11 ## id female race ses schtyp prog read write math science socst ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 0 1 1 1 2 63 65 48 63 56 ## 2 5 0 1 1 1 2 47 40 43 45 31 ## 3 16 0 1 1 1 3 47 31 44 36 36 ## 4 35 1 1 1 2 1 60 54 50 50 51 ## 5 8 1 1 1 1 2 39 44 52 44 48 ## 6 19 1 1 1 1 1 28 46 43 44 51 5.2 Output Most of the time, you should store your data as a csv file. This will ensure that pretty much anyone can take a look at it. If you’re sure that the only users will be other R users (why would you be sure of this?), then feel free to save it as an .Rdata file. ## ------------------------------------------------------------------------ #Saving as an R file save(free2,file=&quot;frl.Rdata&quot;) #Outputting delimited write_csv(free2,&quot;frl.csv&quot;) "],
["using-regression-for-prediction.html", "6 Using Regression for Prediction 6.1 Overview 6.2 Bivariate regression 6.3 Multiple Regression. 6.4 Transformations", " 6 Using Regression for Prediction 6.1 Overview So far, we’ve been using just the simple mean to make predictions. Today, we’ll continue using the simple mean to make predictions, but now in a complicated way. Before, when we calculated conditional means, we did so in certain “groupings” of variables. When we run linear regression, we no longer need to do so. Instead, linear regression allows us to calculate the conditional mean of the outcome at every value of the predictor. If the predictor takes on just a few values, then that’s the number of conditional means that will be calculated. If the predictor is continuous and takes on a large number of values, we’ll still be able to calculate the conditional mean at every one of those values. We’re going to be working with expenditure data from the 2012 administration of the consumer expenditure survey. The first bit of code gets the libraries we need, the data we need, and opens up a codebook for the data. 6.2 Bivariate regression Our first dependent variable will be dining out. Let’s take a look at that variable: summary(cex$dine_out) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0 65.0 325.0 468.1 650.0 9100.0 gg&lt;-ggplot(cex,aes(x=dine_out)) gg&lt;-gg+geom_histogram() gg ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. gg&lt;-ggplot(cex,aes(x=dine_out)) gg&lt;-gg+geom_density() gg Because this variable is pretty non-normally distributed, we may want to think about transforming it. For now, let’s just work with it as-is. Let’s see if people with bigger families spend more on dining out more than those with smaller families. Before, we would have calculated the conditional mean at every level of family size, or in certain groupings of family size. With regression, we simply specify the relationship. #Model 1: simple bivariate regression mod1&lt;-lm(dine_out~fam_size,data=cex) #outcome on left, predictor on right summary(mod1) ## ## Call: ## lm(formula = dine_out ~ fam_size, data = cex) ## ## Residuals: ## Min 1Q Median 3Q Max ## -984.2 -384.7 -150.7 162.8 8574.6 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 372.517 13.911 26.78 &lt; 2e-16 *** ## fam_size 38.230 4.767 8.02 1.24e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 593 on 6822 degrees of freedom ## Multiple R-squared: 0.00934, Adjusted R-squared: 0.009195 ## F-statistic: 64.32 on 1 and 6822 DF, p-value: 1.237e-15 g1&lt;-ggplot(cex, aes(x=fam_size,y=dine_out))+ #specify data and x and y geom_point(shape=1)+ #specify points geom_smooth(method=lm) #ask for lm line g1 pred1&lt;-predict(mod1) #predict using data in memory rmse1&lt;-rmse(cex$dine_out-pred1); rmse ## function(error) ## { ## sqrt(mean(error^2)) ## } What this shows is that as family size increases, the amount spent on dining out increases. For every additional family member, an additional 38.2303747 is predicted to be spent on dining out. The rmse of 592.9362808 gives us a sense of how wrong the model tends to be when using just this one predictor. Quick Exercise Run a regression using a different predictor. Calculate rmse and see if you can beat my score. 6.3 Multiple Regression. Okay, so we can see that this is somewhat predictive, but we can do better. Let’s add in a second variable: whether or not the family is below the poverty line. #Part 2: Multiple regression mod2&lt;-lm(dine_out~fam_size+ pov_cym, #can only take on two values data=cex) summary(mod2) ## ## Call: ## lm(formula = dine_out ~ fam_size + pov_cym, data = cex) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1005.7 -332.1 -144.6 151.6 8529.2 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 114.665 21.129 5.427 5.93e-08 *** ## fam_size 36.239 4.682 7.740 1.14e-14 *** ## pov_cymNot in Poverty 311.219 19.458 15.994 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 582.2 on 6821 degrees of freedom ## Multiple R-squared: 0.04515, Adjusted R-squared: 0.04487 ## F-statistic: 161.3 on 2 and 6821 DF, p-value: &lt; 2.2e-16 pred2&lt;-predict(mod2) rmse(cex$dine_out-pred2) ## [1] 582.1207 So, those who are in poverty spend less on dining out. Alert the media! Quick Exercise Add poverty to your model from above and see what difference it makes. How is your RMSE? Maybe it’s the case that those who spend more on groceries dine out less. Let’s find out: #Model 3: predicting dining out using other variables and grocery spending mod3&lt;-lm(dine_out~ fam_size+ pov_cym+ grocery, data=cex) summary(mod3) ## ## Call: ## lm(formula = dine_out ~ fam_size + pov_cym + grocery, data = cex) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2785.3 -324.8 -125.3 136.9 8570.7 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 62.114475 20.949377 2.965 0.00304 ** ## fam_size -4.026819 5.184420 -0.777 0.43735 ## pov_cymNot in Poverty 257.868962 19.339246 13.334 &lt; 2e-16 *** ## grocery 0.138693 0.008306 16.698 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 570.7 on 6820 degrees of freedom ## Multiple R-squared: 0.08266, Adjusted R-squared: 0.08225 ## F-statistic: 204.8 on 3 and 6820 DF, p-value: &lt; 2.2e-16 g2&lt;-ggplot(cex, aes(x=grocery,y=dine_out))+ geom_point(shape=1)+ geom_smooth(method=lm) g2 Hmm, what happened here? Quick Exercise Use a subset of the cex data with reasonable bounds on both dining out and grocery expenditures. See if the results hold. 6.4 Transformations The big issue as you can see with this data is that the outcome variable isn’t normally distributed: most people spend very little on dining out, while some people spend quite a lot. In situations like this, which are VERY common when dealing with monetary values, we want to take the natural log of the outcome variable. A natural log is the power by which we would have to raise \\(e\\), Euler’s constant, to be that value: \\(e^{ln(x)}=x\\), or \\(ln(e^x)=x\\). Economists just basically take the natural log of everything that’s denominated in dollar terms, which you probably should do as well. You’ll notice in the equations below that I specify the log() of both dining out and grocery spending. #Part 4: Working with transformations mod4&lt;-lm(log(dine_out+1)~ #log of dining out, plus one for zeros +log(grocery+1)+ #log of groceries, plus one again pov_cym+ #poverty fam_size #family size ,data=cex) summary(mod4) ## ## Call: ## lm(formula = log(dine_out + 1) ~ +log(grocery + 1) + pov_cym + ## fam_size, data = cex) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.8373 -0.4087 0.9051 1.5967 7.1674 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.39857 0.22933 1.738 0.0823 . ## log(grocery + 1) 0.41893 0.03538 11.842 &lt;2e-16 *** ## pov_cymNot in Poverty 1.57931 0.08572 18.425 &lt;2e-16 *** ## fam_size 0.01008 0.02198 0.459 0.6464 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.504 on 6820 degrees of freedom ## Multiple R-squared: 0.08598, Adjusted R-squared: 0.08558 ## F-statistic: 213.8 on 3 and 6820 DF, p-value: &lt; 2.2e-16 pred4&lt;-(predict(mod4)) exp(rmse(log(cex$dine_out+1)-pred4)) ## [1] 12.22413 g4&lt;-ggplot(cex, aes(x=grocery,y=exp(pred4),color=pov_cym)) g4&lt;-g4+geom_point(shape=1) g4 When calculating RMSE, I need to work with it in log format. The prediction command will give me back a prediction in log format as well. I take the difference between the two in log format, then exponentiate using the exp command, which means raising \\(e\\) to the power of \\(x\\), \\(e^x\\). #Part 5: Adding income mod5&lt;-lm(log(dine_out+1)~ +log(grocery+1)+ pov_cym+ fam_size+ inclass ,data=cex) summary(mod5) ## ## Call: ## lm(formula = log(dine_out + 1) ~ +log(grocery + 1) + pov_cym + ## fam_size + inclass, data = cex) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.1979 -0.5699 0.7948 1.5897 6.0008 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.70238 0.25098 6.783 1.28e-11 *** ## log(grocery + 1) 0.29036 0.03497 8.303 &lt; 2e-16 *** ## pov_cymNot in Poverty 0.37223 0.14906 2.497 0.012542 * ## fam_size -0.12712 0.02304 -5.518 3.56e-08 *** ## inclass02 -0.06889 0.20307 -0.339 0.734425 ## inclass03 -0.57489 0.19705 -2.917 0.003540 ** ## inclass04 -0.18314 0.21770 -0.841 0.400237 ## inclass05 0.30874 0.21863 1.412 0.157961 ## inclass06 0.80404 0.23002 3.496 0.000476 *** ## inclass07 0.83575 0.23631 3.537 0.000408 *** ## inclass08 1.14419 0.23107 4.952 7.53e-07 *** ## inclass09 1.97654 0.22740 8.692 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.421 on 6812 degrees of freedom ## Multiple R-squared: 0.1468, Adjusted R-squared: 0.1455 ## F-statistic: 106.6 on 11 and 6812 DF, p-value: &lt; 2.2e-16 pred5&lt;-(predict(mod5)) exp(rmse(log(cex$dine_out+1)-pred5 )) ## [1] 11.23037 g5&lt;-ggplot(cex, aes(x=inclass,y=dine_out,group=1))+ geom_point(shape=1)+ geom_smooth(method=lm) g5 "],
["regression-using-a-binary-outcome.html", "7 Regression using a binary outcome", " 7 Regression using a binary outcome You can also run a regression using a binary variable. Let’s recode and then use our cigarettes variable to look at predictors of buying any cigarretes at all. cex$cigs&lt;-0 cex$cigs[cex$cigarettes&gt;0]&lt;-1 mod6&lt;-lm(cigs~educ_ref+ ref_race+ inc_rank+ sex_ref+ fam_type, data=cex) summary(mod6) ## ## Call: ## lm(formula = cigs ~ educ_ref + ref_race + inc_rank + sex_ref + ## fam_type, data = cex) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.5228 -0.2119 -0.1498 -0.0431 0.9906 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.258750 0.108260 2.390 0.016877 * ## educ_ref10 -0.101923 0.109432 -0.931 0.351693 ## educ_ref11 0.006414 0.108639 0.059 0.952923 ## educ_ref12 0.003200 0.107796 0.030 0.976321 ## educ_ref13 -0.045853 0.107922 -0.425 0.670947 ## educ_ref14 -0.023866 0.108584 -0.220 0.826037 ## educ_ref15 -0.135594 0.108102 -1.254 0.209781 ## educ_ref16 -0.166795 0.108931 -1.531 0.125777 ## educ_ref17 -0.154400 0.111589 -1.384 0.166523 ## ref_race2 -0.028553 0.015545 -1.837 0.066286 . ## ref_race3 0.119595 0.073066 1.637 0.101723 ## ref_race4 -0.036712 0.023509 -1.562 0.118429 ## ref_race5 0.001864 0.076077 0.024 0.980456 ## ref_race6 -0.026429 0.041673 -0.634 0.525969 ## inc_rank -0.049425 0.021480 -2.301 0.021433 * ## sex_ref2 -0.036232 0.010258 -3.532 0.000415 *** ## fam_type2 0.011057 0.024607 0.449 0.653195 ## fam_type3 -0.003627 0.017747 -0.204 0.838057 ## fam_type4 0.051879 0.022608 2.295 0.021784 * ## fam_type5 0.049327 0.026059 1.893 0.058421 . ## fam_type6 0.019565 0.050344 0.389 0.697569 ## fam_type7 0.031451 0.026456 1.189 0.234563 ## fam_type8 -0.005898 0.015284 -0.386 0.699560 ## fam_type9 0.142249 0.016964 8.385 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3708 on 5668 degrees of freedom ## (1132 observations deleted due to missingness) ## Multiple R-squared: 0.05822, Adjusted R-squared: 0.05439 ## F-statistic: 15.23 on 23 and 5668 DF, p-value: &lt; 2.2e-16 g4&lt;-ggplot(cex,aes(x=fam_type,y=cigs,group=1))+ geom_jitter() g4 "],
["thinking-about-regression-for-prediction.html", "8 Thinking about regression for prediction", " 8 Thinking about regression for prediction You MUST remember: correlation is not causation. All you can pick up on using this tool is associations, or common patterns. You can’t know whether one thing causes another. Remember that the left hand side variable could just as easily be on the right hand side. "],
["scatterplots.html", "9 Scatterplots 9.1 Setup 9.2 Bivariate Regression 9.3 Multiple Regression", " 9 Scatterplots Scatterplots are the best way to present data that has a continuous response variable. When creating scatterplots, the idea is to show ALL of the data, and then show how your model is summarizing the relationships in that data. 9.1 Setup The code for today starts with the normal set of preliminaries, opening up the cex.RData dataset and creating a codebook. 9.2 Bivariate Regression We begin with a simple model of grocery food spending as a function of income rank. As we found out last week, grocery food spending has a long right tail, and is a good candidate for a log transformation. We’ll be using that log transformation throughout. Our first step should be to plot the data. Today, we’ll be using the ggplot2 library, which is a highly functional implementation of what’s known as the grammar of graphics. In a very small nutshell, the grammar of graphics refers to laying out a graphic in a series of layers. For our first scatterplot, we first specify the data that we’ll be drawing on, then the “aesthetic” of the graphic, which will be based on our x and y variables from our regression. I then specify the first layer, which is a series of points defined by the intersection of the x and y variables. #Plot Spending vs. Income g1&lt;-ggplot(data=cex, aes(x=inc_rank,y=(grocery_food+1)) ) g1&lt;-g1+geom_point() # Add points at x and y g1 ## Warning: Removed 1134 rows containing missing values (geom_point). This first graphic shows the basic problem with the data, which is the highly non-normal shape of the outcome variable. No matter! We’ll keep moving forward. The next line of code adds a layer with a regression line to the plot. g1&lt;-g1+geom_smooth(method=&quot;lm&quot;) g1 ## Warning: Removed 1134 rows containing non-finite values (stat_smooth). ## Warning: Removed 1134 rows containing missing values (geom_point). It’s also really hard to see. We can use conditional means to help out with that problem. Let’s get the average amount of grocery spending at every percentile level of inc_rank. Notice the use of round to get income percentiles that are at two digits only. cex_sum&lt;-cex%&gt;%mutate(inc_rank_r=round(inc_rank))%&gt;%group_by(inc_rank_r)%&gt;%summarize(groc_mean=mean(grocery)) g1a&lt;-ggplot(cex_sum,aes(x=inc_rank_r,y=groc_mean)) g1a&lt;-g1a+geom_point() g1a ## Warning: Removed 1 rows containing missing values (geom_point). We can add a regression line to this simpler data g1a&lt;-g1a+geom_smooth(method=lm) # Add a line g1a ## Warning: Removed 1 rows containing non-finite values (stat_smooth). ## Warning: Removed 1 rows containing missing values (geom_point). As we figured out in class last week, a simple line plot doesn’t really do it. We need to transform that dependent variable. Luckily, ggplot has this kind of transformation built in. g1&lt;-ggplot(data=cex, aes(x=inc_rank,y=(grocery_food+1)) ) my.breaks=c(0,100,500,1000,5000,10000,20000) #Change the scale g1&lt;-g1+scale_y_continuous(trans=&quot;log&quot;,breaks=my.breaks) g1&lt;-g1+geom_point() # Add points g1&lt;-g1+geom_smooth(method=lm) #Add a Line g1&lt;-g1+ylab(&quot;Grocery Spending&quot;)+xlab(&quot;Income Percentile (0-1)&quot;) #Nice labels g1 ## Warning: Removed 1134 rows containing non-finite values (stat_smooth). ## Warning: Removed 1134 rows containing missing values (geom_point). Notice how different the steps are on the y axis now. But this shows that our line actually fits the data much better once we work on the log scale. We’re now ready to go ahead and run our first model based on what we’ve learned. #First model m1&lt;-lm(log(grocery_food+1)~inc_rank,data=cex);summary(m1) ## ## Call: ## lm(formula = log(grocery_food + 1) ~ inc_rank, data = cex) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.3716 -0.3002 0.0714 0.4495 2.8556 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.2657005 0.0230229 272.15 &lt;2e-16 *** ## inc_rank 0.0111849 0.0003982 28.09 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8775 on 5702 degrees of freedom ## (1134 observations deleted due to missingness) ## Multiple R-squared: 0.1215, Adjusted R-squared: 0.1214 ## F-statistic: 788.9 on 1 and 5702 DF, p-value: &lt; 2.2e-16 Quick Exercise Create a similar graphic, but this time use other store expenditures as the dependent variable. Run a regression with store expenditures as the dv and income rank as the only independent variable. 9.3 Multiple Regression The next step is to add covariates. I’ll be working with the variable childage which is a factor that summarizes the ages of kids in the family. I’m going to set the color of the markers by the childage factor. g2&lt;-ggplot(data=cex, aes(x=inc_rank,y=(grocery_food+1),color=as.numeric(childage),alpha=as.numeric(childage)) #notice the color option ) g2&lt;-g2+scale_y_continuous(trans=&quot;log&quot;,breaks=my.breaks) g2&lt;-g2+geom_point(size=1) g2 ## Warning: Removed 1134 rows containing missing values (geom_point). Our first graphic is a bit complex, but shows that those in the “0” category, with no kids, have consistently lower spending on groceries across income levels. So let’s drop that group and see what the graphic looks like for those with at least one kid of any age. g2a&lt;-ggplot(data=filter(cex,as.character(cex$childage)!=&quot;0&quot;), aes(x=inc_rank,y=(grocery_food+1),color=childage) ) g2a&lt;-g2a+scale_y_continuous(trans=&quot;log&quot;,breaks=my.breaks,limits=c(100,8000)) #Changed overall scale g2a&lt;-g2a+geom_point(size=2) #Bigger points g2a&lt;-g2a+scale_colour_hue(l=50) #Darker palette g2a ## Warning: Removed 1212 rows containing missing values (geom_point). Notice the higher spending levels at every income level among those in the 4 and 5 categories. #Model 2: with kids mod2&lt;-lm(log(grocery_food+1)~inc_rank+childage, data=cex,na.action=na.exclude);summary(mod2) ## ## Call: ## lm(formula = log(grocery_food + 1) ~ inc_rank + childage, data = cex, ## na.action = na.exclude) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.1023 -0.2740 0.0760 0.4065 3.0520 ## ## Coefficients: ## Estimate ## (Intercept) 6.1644855 ## inc_rank 0.0094850 ## childageAll less 6 0.3310502 ## childageOldest bt 6 and 11 0.5116967 ## childageAll children between 6 and 11 0.3977986 ## childageOldest child between 12 and 17 and at least one child less than 12 0.6975020 ## childageAll children between 12 and 17 0.3735577 ## childageOldest child greater than 17 and at least one child less than 17 0.6158860 ## childageAll children greater than 17 one under 6 0.3752501 ## Std. Error ## (Intercept) 0.0228141 ## inc_rank 0.0003958 ## childageAll less 6 0.0436714 ## childageOldest bt 6 and 11 0.0565955 ## childageAll children between 6 and 11 0.0580077 ## childageOldest child between 12 and 17 and at least one child less than 12 0.0476091 ## childageAll children between 12 and 17 0.0529915 ## childageOldest child greater than 17 and at least one child less than 17 0.0589702 ## childageAll children greater than 17 one under 6 0.0384561 ## t value ## (Intercept) 270.205 ## inc_rank 23.967 ## childageAll less 6 7.580 ## childageOldest bt 6 and 11 9.041 ## childageAll children between 6 and 11 6.858 ## childageOldest child between 12 and 17 and at least one child less than 12 14.651 ## childageAll children between 12 and 17 7.049 ## childageOldest child greater than 17 and at least one child less than 17 10.444 ## childageAll children greater than 17 one under 6 9.758 ## Pr(&gt;|t|) ## (Intercept) &lt; 2e-16 ## inc_rank &lt; 2e-16 ## childageAll less 6 4.00e-14 ## childageOldest bt 6 and 11 &lt; 2e-16 ## childageAll children between 6 and 11 7.74e-12 ## childageOldest child between 12 and 17 and at least one child less than 12 &lt; 2e-16 ## childageAll children between 12 and 17 2.01e-12 ## childageOldest child greater than 17 and at least one child less than 17 &lt; 2e-16 ## childageAll children greater than 17 one under 6 &lt; 2e-16 ## ## (Intercept) *** ## inc_rank *** ## childageAll less 6 *** ## childageOldest bt 6 and 11 *** ## childageAll children between 6 and 11 *** ## childageOldest child between 12 and 17 and at least one child less than 12 *** ## childageAll children between 12 and 17 *** ## childageOldest child greater than 17 and at least one child less than 17 *** ## childageAll children greater than 17 one under 6 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8463 on 5695 degrees of freedom ## (1134 observations deleted due to missingness) ## Multiple R-squared: 0.1838, Adjusted R-squared: 0.1826 ## F-statistic: 160.3 on 8 and 5695 DF, p-value: &lt; 2.2e-16 Now let’s take a look at this model plotted against the actual data. I’m going to use the alpha setting to make the dots smaller. I’m also going to make the dots smaller. cex&lt;-cex%&gt;%mutate(mod2_pred=exp(fitted(mod2))) g3&lt;-ggplot(cex,aes(x=inc_rank,y=grocery_food)) g3&lt;-g3+geom_point(alpha=.2,size=.75) g3&lt;-g3+scale_y_continuous(trans=&quot;log&quot;,breaks=my.breaks) g3&lt;-g3+geom_smooth(data=cex,(aes(x=inc_rank,y=mod2_pred))) g3 ## Warning: Transformation introduced infinite values in continuous y-axis ## `geom_smooth()` using method = &#39;gam&#39; ## Warning: Removed 1134 rows containing non-finite values (stat_smooth). ## Warning: Removed 1134 rows containing missing values (geom_point). As we add more variables to the model, it can get more difficult to plot relationships. One very good option is to plot lines based on a hypothetical set of data. Below, I create a hypothetical set of data that include values of income across the range of income, and includes values for every level of childage. #Prediction, then plotting hypo.data&lt;-data.frame(expand.grid( #range of income inc_rank=seq(min(cex$inc_rank,na.rm=TRUE), max(cex$inc_rank,na.rm=TRUE), length=100), #All levels of childage childage=levels(cex$childage) ) ) Now, using my estimates from model 2, I predict what would happen to these hypothetical individuals. Once I’ve got my prediction, I transform it back out of the log scale into the “response” level of dollars. #Predict using mod2 on hypothetical data hypo.pred&lt;-predict(mod2, newdata=hypo.data, interval=&quot;prediction&quot;, se.fit=TRUE ) hypo.data&lt;-data.frame(hypo.data,hypo.pred$fit) #Add fit to the dataset hypo.data$groc.p&lt;-exp(hypo.data$fit) #exponentiate fit hypo.data$groc.lwr&lt;-exp(hypo.data$lwr) #exponentiate lower boundary hypo.data$groc.upr&lt;-exp(hypo.data$upr) #exponentiate upper boundary Now we can plot the result, using the geom_smooth layer to give us lines for every level of childage. g4&lt;-ggplot(data=hypo.data,aes(x=inc_rank,y=groc.p,color=fct_reorder(f=childage,-groc.p))) #notice color g4&lt;-g4+geom_smooth(method=lm,se=FALSE) g4 To show this in the data we can break it out for every type of child age grouping. ## Resort child age for graphic cex&lt;-cex%&gt;%mutate(childage=fct_reorder(f=childage,-grocery_food)) g5&lt;-ggplot(cex,aes(x=inc_rank,y=grocery_food,color=childage)) g5&lt;-g5+geom_point(alpha=.5) g5&lt;-g5+geom_smooth(method=&quot;lm&quot;,color=&quot;black&quot;) g5&lt;-g5+facet_wrap(~childage,ncol=3) g5&lt;-g5+scale_y_continuous(trans=&quot;log&quot;,breaks=my.breaks) g5&lt;-g5+xlab(&quot;Income Rank, 0-100&quot;)+ylab(&quot;Grocery Spending&quot;) g5&lt;-g5+theme(legend.position=&quot;none&quot;) g5 ## Warning: Transformation introduced infinite values in continuous y-axis ## Warning: Transformation introduced infinite values in continuous y-axis ## Warning: Removed 1190 rows containing non-finite values (stat_smooth). ## Warning: Removed 1134 rows containing missing values (geom_point). (Not so) Quick Exercise Run a different model, this time using fam_type as a factor variable. Plot the estimated relationship between income rank and food spending for each family type. "],
["appendix-appendix.html", "(APPENDIX )Appendix 9.4 List of Lists of Datasets", " (APPENDIX )Appendix 9.4 List of Lists of Datasets 9.4.1 Kaggle Kaggle hosts competitions for predicting various outcomes. Companies supply their data to Kaggle and then provide awards for the teams that are best able to predict outcomes. They have a large number of publicly available datasets: https://www.kaggle.com/datasets 9.4.2 Data is Plural A google docs spreadsheet with links to a huge number of datasets https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/edit#gid=0 9.4.3 Tableau’s list of sports datasets https://public.tableau.com/s/blog/2014/03/where-find-sports-data 9.4.4 Milne library list of public datasets http://libguides.geneseo.edu/c.php?g=67454&amp;p=434785#s-lg-box-1300425 9.4.5 Data.gov All publicly available government datasets. https://www.data.gov/ 9.4.6 Awesome public datasets on GitHub https://github.com/caesar0301/awesome-public-datasets#awesome-public-datasets 9.4.7 Google BigQuery Data https://cloud.google.com/bigquery/public-data/ 9.4.8 ASD Free The website says it all “obsessively-detailed instructions to analyze survey data for free with the r language, the survey package, monetdblite.” Includes the survey of consumer expenditures, nationaly longitudinal survey of youth, european social survey and so. much. more. http://www.asdfree.com/ "]
]
